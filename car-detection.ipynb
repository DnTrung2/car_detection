{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":38348,"sourceType":"datasetVersion","datasetId":30084},{"sourceId":6224546,"sourceType":"datasetVersion","datasetId":3575098}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tfr\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\n","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:59:48.942139Z","iopub.execute_input":"2023-08-06T17:59:48.942502Z","iopub.status.idle":"2023-08-06T17:59:48.949034Z","shell.execute_reply.started":"2023-08-06T17:59:48.942473Z","shell.execute_reply":"2023-08-06T17:59:48.947854Z"},"trusted":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":"TRAIN_DIR = \"/kaggle/input/stanford-cars-dataset/cars_train\"\nTEST_DIR = \"/kaggle/input/stanford-cars-dataset/cars_test\"\ncars_train_dir = os.path.join(TRAIN_DIR, \"cars_train\")\ncars_test_dir = os.path.join(TEST_DIR, \"cars_test\")","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:59:48.951220Z","iopub.execute_input":"2023-08-06T17:59:48.951651Z","iopub.status.idle":"2023-08-06T17:59:48.962590Z","shell.execute_reply.started":"2023-08-06T17:59:48.951608Z","shell.execute_reply":"2023-08-06T17:59:48.961322Z"},"trusted":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"print(\"Cars Train: \", len(os.listdir(cars_train_dir)))\nprint(\"Cars Test: \", len(os.listdir(cars_test_dir)))\n\n\ncars_train_files = os.listdir(cars_train_dir)\nprint(cars_train_files[:5])\n\ncars_test_files = os.listdir(cars_test_dir)\nprint(cars_test_files[:5])","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:59:48.964714Z","iopub.execute_input":"2023-08-06T17:59:48.965835Z","iopub.status.idle":"2023-08-06T17:59:49.657036Z","shell.execute_reply.started":"2023-08-06T17:59:48.965799Z","shell.execute_reply":"2023-08-06T17:59:49.656040Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Cars Train:  8144\nCars Test:  8041\n['05938.jpg', '06122.jpg', '04168.jpg', '02371.jpg', '04377.jpg']\n['05938.jpg', '06122.jpg', '04168.jpg', '02371.jpg', '04377.jpg']\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import scipy.io as sio\nimport pandas as pd\n\n# Replace 'path_to_file' with the actual path to your .mat file.\nmat_data = sio.loadmat('/kaggle/input/stanford-cars-dataset/cars_annos.mat')\nprint(mat_data.keys())\n\ndata = mat_data['annotations']\n\n# Extract individual arrays from the 'data' variable\nimage_paths = data['relative_im_path'].squeeze()   # Assuming 'relative_im_path' is the field name for image paths\nx_min = data['bbox_x1'].squeeze()                  # Assuming 'bbox_x1' is the field name for x_min\ny_min = data['bbox_y1'].squeeze()                  # Assuming 'bbox_y1' is the field name for y_min\nx_max = data['bbox_x2'].squeeze()                  # Assuming 'bbox_x2' is the field name for x_max\ny_max = data['bbox_y2'].squeeze()                  # Assuming 'bbox_y2' is the field name for y_max\n\n# Create a DataFrame from the extracted arrays\nannotations_df = pd.DataFrame({\n    'image_path': image_paths,\n    'x_min': x_min,\n    'y_min': y_min,\n    'x_max': x_max,\n    'y_max': y_max\n})\n","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:59:49.660155Z","iopub.execute_input":"2023-08-06T17:59:49.662197Z","iopub.status.idle":"2023-08-06T17:59:50.202611Z","shell.execute_reply.started":"2023-08-06T17:59:49.662169Z","shell.execute_reply":"2023-08-06T17:59:50.201666Z"},"trusted":true},"outputs":[{"name":"stdout","text":"dict_keys(['__header__', '__version__', '__globals__', 'annotations', 'class_names'])\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"annotations_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:59:50.205264Z","iopub.execute_input":"2023-08-06T17:59:50.205707Z","iopub.status.idle":"2023-08-06T17:59:50.235050Z","shell.execute_reply.started":"2023-08-06T17:59:50.205672Z","shell.execute_reply":"2023-08-06T17:59:50.233903Z"},"trusted":true},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"             image_path    x_min   y_min    x_max    y_max\n0  [car_ims/000001.jpg]  [[112]]   [[7]]  [[853]]  [[717]]\n1  [car_ims/000002.jpg]   [[48]]  [[24]]  [[441]]  [[202]]\n2  [car_ims/000003.jpg]    [[7]]   [[4]]  [[277]]  [[180]]\n3  [car_ims/000004.jpg]   [[33]]  [[50]]  [[197]]  [[150]]\n4  [car_ims/000005.jpg]    [[5]]   [[8]]   [[83]]   [[58]]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>x_min</th>\n      <th>y_min</th>\n      <th>x_max</th>\n      <th>y_max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[car_ims/000001.jpg]</td>\n      <td>[[112]]</td>\n      <td>[[7]]</td>\n      <td>[[853]]</td>\n      <td>[[717]]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[car_ims/000002.jpg]</td>\n      <td>[[48]]</td>\n      <td>[[24]]</td>\n      <td>[[441]]</td>\n      <td>[[202]]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[car_ims/000003.jpg]</td>\n      <td>[[7]]</td>\n      <td>[[4]]</td>\n      <td>[[277]]</td>\n      <td>[[180]]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[car_ims/000004.jpg]</td>\n      <td>[[33]]</td>\n      <td>[[50]]</td>\n      <td>[[197]]</td>\n      <td>[[150]]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[car_ims/000005.jpg]</td>\n      <td>[[5]]</td>\n      <td>[[8]]</td>\n      <td>[[83]]</td>\n      <td>[[58]]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"# Convert the bounding box coordinates to single values\nannotations_df['x_min'] = annotations_df['x_min'].astype(int)\nannotations_df['y_min'] = annotations_df['y_min'].astype(int)\nannotations_df['x_max'] = annotations_df['x_max'].astype(int)\nannotations_df['y_max'] = annotations_df['y_max'].astype(int)\n\nannotations_df['image_path'] = annotations_df['image_path'].apply(lambda x: x[0])\nannotations_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:59:50.239398Z","iopub.execute_input":"2023-08-06T17:59:50.239682Z","iopub.status.idle":"2023-08-06T17:59:50.330359Z","shell.execute_reply.started":"2023-08-06T17:59:50.239658Z","shell.execute_reply":"2023-08-06T17:59:50.328846Z"},"trusted":true},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"           image_path  x_min  y_min  x_max  y_max\n0  car_ims/000001.jpg    112      7    853    717\n1  car_ims/000002.jpg     48     24    441    202\n2  car_ims/000003.jpg      7      4    277    180\n3  car_ims/000004.jpg     33     50    197    150\n4  car_ims/000005.jpg      5      8     83     58","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>x_min</th>\n      <th>y_min</th>\n      <th>x_max</th>\n      <th>y_max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>car_ims/000001.jpg</td>\n      <td>112</td>\n      <td>7</td>\n      <td>853</td>\n      <td>717</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>car_ims/000002.jpg</td>\n      <td>48</td>\n      <td>24</td>\n      <td>441</td>\n      <td>202</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>car_ims/000003.jpg</td>\n      <td>7</td>\n      <td>4</td>\n      <td>277</td>\n      <td>180</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>car_ims/000004.jpg</td>\n      <td>33</td>\n      <td>50</td>\n      <td>197</td>\n      <td>150</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>car_ims/000005.jpg</td>\n      <td>5</td>\n      <td>8</td>\n      <td>83</td>\n      <td>58</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\ndef preprocess_image(image_path):\n    # Load the image from the provided path\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Resize the image to the required input shape (e.g., 224x224)\n    img = cv2.resize(img, (224, 224))\n    \n   \n\n    # Normalize pixel values to the range [0, 1]\n    img = img.astype(np.float32) / 255.0\n\n    return img\n\n# Preprocess the images and create the training dataset\nimages = [preprocess_image(f\"/kaggle/input/stanford-cars-dataset/cars_train/cars_train/{path}\") for path in cars_train_files]\nimages = np.array(images)\n\n# Extract the bounding box coordinates (x_min, y_min, x_max, y_max) from the annotations DataFrame\nx_min = annotations_df['x_min'].values\ny_min = annotations_df['y_min'].values\nx_max = annotations_df['x_max'].values\ny_max = annotations_df['y_max'].values\n\n# Normalize the bounding box coordinates to the range [0, 1]\nx_center = (x_min + x_max) / 2 / 224.0\ny_center = (y_min + y_max) / 2 / 224.0\nwidth = (x_max - x_min) / 224.0\nheight = (y_max - y_min) / 224.0\n\n# Combine the bounding box coordinates into a single array\nannotations = np.column_stack((x_center, y_center, width, height))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:59:50.336077Z","iopub.execute_input":"2023-08-06T17:59:50.338809Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Assuming you have already loaded the data and annotations as shown in your previous code\n\n# Calculate the number of images you want to keep\ndesired_num_images = 3000  # Replace with the desired number of images\n\n# Check the current number of images and annotations\nnum_images = len(images)\nnum_annotations = len(annotations)\n\n# Ensure that the number of desired images is not greater than the current number of images\ndesired_num_images = min(desired_num_images, num_images)\n\n# Randomly select a subset of indices for the annotations\nselected_indices = np.random.choice(num_annotations, desired_num_images, replace=False)\n\n# Create new annotations and images arrays with the selected subset\nselected_images = images[:desired_num_images]\nselected_annotations = annotations[selected_indices]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming you have already loaded the data and annotations as shown in your previous code\n# Assuming annotations have the format (x_min, y_min, x_max, y_max) for each bounding box\n# You can modify this code as needed based on your specific annotation format.\n\n# Define the number of grid cells in the x and y directions\ngrid_size = 7\n\ndef convert_annotations_to_yolo_format(annotations, input_shape, grid_size):\n    image_width, image_height = input_shape[:2]\n    grid_width = image_width / grid_size\n    grid_height = image_height / grid_size\n\n    yolo_annotations = np.zeros((len(annotations), grid_size, grid_size, 6))\n\n    for i, (x_min, y_min, x_max, y_max) in enumerate(annotations):\n        x_center = (x_min + x_max) / 2\n        y_center = (y_min + y_max) / 2\n        width = x_max - x_min\n        height = y_max - y_min\n\n        grid_x = int(x_center / grid_width)\n        grid_y = int(y_center / grid_height)\n\n        yolo_annotations[i, grid_y, grid_x, 0] = x_center / image_width\n        yolo_annotations[i, grid_y, grid_x, 1] = y_center / image_height\n        yolo_annotations[i, grid_y, grid_x, 2] = width / image_width\n        yolo_annotations[i, grid_y, grid_x, 3] = height / image_height\n        yolo_annotations[i, grid_y, grid_x, 4] = 1  # Confidence score, assuming all boxes are present\n        yolo_annotations[i, grid_y, grid_x, 5] = 1  # Class score, assuming only one class (car)\n\n    return yolo_annotations\n\n# Convert annotations to YOLO format with the new grid_size\n\n# Convert annotations to YOLO format\ninput_shape=(224,224,3)\nyolo_annotations_train = convert_annotations_to_yolo_format(selected_annotations, input_shape,grid_size)\n# Split data into training and validation sets\n# Define your split ratio as needed (e.g., 80% training, 20% validation)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass YOLOv3(nn.Module):\n    def __init__(self, num_classes=1):\n        super(YOLOv3, self).__init__()\n        \n        # Backbone network\n        self.conv1 = nn.Conv2d(3, 32, 3) \n        self.conv2 = nn.Conv2d(32, 64, 3)\n        #...\n        \n        # Output layers\n        self.conv_out1 = nn.Conv2d(1024, 1024, 3)\n        self.conv_out2 = nn.Conv2d(768, 768, 3)\n        self.conv_out3 = nn.Conv2d(384, 384, 3)\n        \n        self.pred1 = nn.Conv2d(1024, 5+num_classes, 1)\n        self.pred2 = nn.Conv2d(768, 5+num_classes, 1) \n        self.pred3 = nn.Conv2d(384, 5+num_classes, 1)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        #...\n        \n        out1 = self.pred1(self.conv_out1(x))\n        out2 = self.pred2(self.conv_out2(x)) \n        out3 = self.pred3(self.conv_out3(x))\n        \n        return out1, out2, out3\n\n# Define the new grid size\ngrid_size = 7\n\n# Assuming num_classes is defined with the correct value\nnum_classes = 1\n\n# Create the YOLO model with the updated grid size\nyolo_model = YOLOv3(num_classes)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\n\n# Assuming you have already loaded the data and annotations as shown in your previous code\nx_train,x_test,y_train,y_test = train_test_split(selected_images,yolo_annotations_train,train_size=0.8,random_state=42)\n# Split data into training and validation sets\n# Define your split ratio as needed (e.g., 80% training, 20% validation)\nprint(\"Training data shapes:\", x_train.shape, y_train.shape)\nprint(\"Validation data shapes:\", x_test.shape, y_test.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(x_train.dtype)\nprint(y_train.dtype)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True,\n    fill_mode='nearest'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stopping_callback = EarlyStopping(monitor='val_accuracy', patience=5, mode='auto', min_delta=0.001, baseline=0.7, restore_best_weights=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n\n# Convert to float32\nx_train = x_train.astype(np.float32)\ny_train = y_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\ny_test = y_test.astype(np.float32)\n\n# Convert NumPy arrays to PyTorch tensors\nx_train_tensor = torch.tensor(x_train)\ny_train_tensor = torch.tensor(y_train)\nx_test_tensor = torch.tensor(x_test)\ny_test_tensor = torch.tensor(y_test)\n\n# Create DataLoaders for training and validation data\ntrain_dataset = TensorDataset(x_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\nval_dataset = TensorDataset(x_test_tensor, y_test_tensor)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\n\nbatch_size=32\n\n\ncriterion = nn.MSELoss()  # Use appropriate loss function for your task\noptimizer = torch.optim.Adam(yolo_model.parameters(), lr=0.001)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nyolo_model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 10\nfor epoch in range(num_epochs):\n    yolo_model.train()\n    total_loss = 0\n\n    for batch_x, batch_y in train_loader:\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n\n        optimizer.zero_grad()\n        out1, out2, out3 = yolo_model(batch_x)\n        loss = criterion(out1, batch_y) + criterion(out2, batch_y) + criterion(out3, batch_y)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    # Compute the average loss for the epoch\n    avg_loss = total_loss / len(train_loader)\n\n    # Update the learning rate using the scheduler\n    scheduler.step(avg_loss)\n\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image=np.expand_dims(preprocess_image('/kaggle/input/stanford-cars-dataset/cars_train/cars_train/00001.jpg'),axis=0)\nbounder=yolo_model.predict(image)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_car(image_path):\n    image = cv2.imread(image_path)\n    image = cv2.resize(image, (224, 224))  # Assuming the input size of the model is (224, 224)\n    image = image / 255.0  # Normalize the image\n    image = np.expand_dims(image, axis=0)  # Add batch dimension\n\n    prediction =yolo_model.predict(image)\n    if prediction[0, :, :, 4].max() > 0.5:\n        return True\n    else:\n        return False\nimages_directory='/kaggle/input/stanford-cars-dataset/cars_test/cars_test'\n\n# Loop through all images in the directory\nfor filename in os.listdir(images_directory):\n    if filename.endswith('.jpg'):\n        image_path = os.path.join(images_directory, filename)\n        car_present = predict_car(image_path)\n        print(f\"Image: {filename}, Car Present: {car_present}\")\n\n        if car_present:\n            print(\"Car detected. Stopping the loop.\")\n            break","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}